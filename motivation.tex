\section{Motivation}

Artificial neural networks are motivated by the learning capabilities of the human brain which consists of neurons interconnected by synapses. In fact -- at least theoretically -- they are able to learn any given mapping up to arbitrary accuracy \cite{HornikStinchcombeWhite:1989}. In addition, they allow to easily incorporate prior knowledge about the task into the network architecture. As result, in 1989, LeCun et al. introduced convolutional neural networks for application in computer vision \cite{LeCunBoserDenkerHenderson:1989}.

Convolutional neural networks use images directly as input. Instead of handcrafted features, convolutional neural networks are used to automatically learn a hierarchy of features which can then be used for classification purposes. This is accomplished by successively convolving the input image with learned filters to build up a hierarchy of feature maps. The hierarchical approach allows to learn more complex, as well as translation and distortion invariant, features in higher layers.

% When discussing the learning of feature hierarchies, we are usually talking about deep learning, that is training deep neural networks. Here, deep neural networks usually refer to neural networks with more than 3 layers. To date, deep learning is still considered challenging \cite{Bengio:2009}. 

% Due to the constrained architecture of convolutional neural networks\footnote{Convolutional neural networks as introduced in \cite{LeCunBoserDenkerHenderson:1989} make use of weight sharing as introduced in section \ref{subsubsec:weight-sharing} which reduces the complexity and size of the network and allows to train deep architectures.}, they allow to train deep architectures with traditional methods\footnote{Usually this includes gradient descent optimization as discussed in section \ref{subsubsec:parameter-optimization} as well as error backpropagation as introduced in section \ref{subsubsec:error-backpropagation} to evaluate the gradient of a chosen error function.}. This allows a convolutional neural network to learn feature hierarchies, that means that the convolutional neural network is used to learn a hierarchy of task specific features which can then be used for classification using traditional multilayer perceptrons\footnote{The multilayer perceptron is discussed in detail in section \ref{subsec:multilayer-perceptron}.}. This property is a huge advantage of convolutional neural networks over fully-connected multilayer perceptrons where learning deep architectures is considered difficult \cite{Bengio:2009}.

In contrast to traditional multilayer perceptrons, where deep learning is considered difficult \cite{Bengio:2009}, deep convolutional neural networks can be trained more easily using traditional methods\footnote{Here, traditional methods refers to gradient descent for parameter optimization combined with error backpropagation as discussed in section \ref{subsec:supervised-training}.}. This property is due to the constrained architecture\footnote{Using weight sharing as discussed in section \ref{subsubsec:weight-sharing}, the actual model complexity is reduced.} of convolutional neural networks which is specific to input for which discrete convolution is defined, such as images. Nevertheless, deep learning of convolutional neural networks is an active area of research, as well.

As with multilayer perceptrons, convolutional neural networks still have some disadvantages when compared to other popular machine learning techniques as for example Support Vector Machines as their internal operation is not well understood \cite{ZeilerFergus:2013}. Using deconvolutional neural networks proposed in \cite{ZeilerKrishnanTaylorFergus:2010}, this problem is addressed in \cite{ZeilerFergus:2013}. The approach described in \cite{ZeilerFergus:2013} allows the visualization of feature activations in higher layers of the network and can be used to give further insights into the internal operation of convolutional neural networks.

\subsection{Bibliographical Notes}

Although this paper briefly introduces the basic notions of neural networks as well as network training, this topic is far too extensive to be covered in detail. For a detailed discussion of neural networks and their training several textbooks are available \cite{Bishop:1995,Bishop:2006,Haykin:2005}.

The convolutional neural network was originally proposed in \cite{LeCunBoserDenkerHenderson:1989} for the task of ZIP code recognition. Both convolutional neural networks as well as traditional multilayer perceptrons were excessively applied to character recognition and handwritten digit recognition \cite{LeCunBottouBengioHaffner:1998}. Training was initially based on error backpropagation \cite{RumelhartHintonWilliams:1986} and gradient descent.

The original convolutional neural network is based on weight sharing which was proposed in \cite{RumelhartHintonWilliams:1986}. An extension of weight sharing called soft weight sharing is discussed in \cite{NowlanHinton:1992}. Recent implementations make use of other regularization techniques as for example dropout \cite{HintonSrivastavaKrizhevskySutskeverSalakhutdinov:2012}.

Although the work by Hinton et al. in 2006 \cite{HintonOsindero:2006} can be considered as breakthrough in deep learning -- as it allows unsupervised training of neural networks -- deep learning is still considered difficult \cite{Bengio:2009}. A thorough discussion of deep learning including recent research is given in \cite{Bengio:2009} as well as \cite{LarochelleBengioLouradourLamblinBottou:2009,GlorotBengio:2010,BengioLeCun:2007}. Additional research on this topic includes discussion on activation functions as well as the effect of unsupervised pre-training \cite{ErhanManzagolBengioVincent:2009,ErhanBengioCourvilleManzagolVincentBengio:2010,GlorotBordesBengio:2011}.

Recent architectural changes of convolutional neural networks are discussed in detail in \cite{JarrettKavukcuogluRanzatoLeCun:2009} and \cite{LeCunKavukvuogluFarabet:2010}. Recent success of convolutional neural networks is reported in \cite{KrizhevskySutskeverHinton:2012} and \cite{CiresanMeierSchmidhuber:2012}.

This paper is mainly motivated by the experiments in \cite{ZeilerFergus:2013}. Based on deconvolutional neural networks \cite{ZeilerKrishnanTaylorFergus:2010}, the authors of \cite{ZeilerFergus:2013} propose a visualization technique allowing to visualize feature activations of higher layers. 